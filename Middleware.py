"""
To create a middleware you use the decorator @app.middleware("http") on top of a function.

The middleware function receives:

The request.
A function call_next that will receive the request as a parameter.
This function will pass the request to the corresponding path operation.
Then it returns the response generated by the corresponding path operation.
You can then further modify the response before returning it.
"""

# Import necessary libraries
import time 
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse

# Create the FastAPI app instance
app = FastAPI()

# Middleware to measure processing time
@app.middleware("http")
async def add_process_time_header(request: Request, call_next):
    # Capture the start time before handling the request
    start_time = time.perf_counter()
    
    # Call the next handler in the chain (endpoint)
    print(f"now request is  {request} and call_next is  {call_next}")
    response = await call_next(request)   # call_next is a special function provided by FastAPI in middleware
    print(f"now response is  {response}")
    
    # Calculate the processing time
    process_time = time.perf_counter() - start_time
    
    # Add the processing time as a custom header to the response
    response.headers["X-Process-Time"] = str(process_time)
    
    # Return the modified response
    return response

# Define a sample endpoint
@app.get("/")
async def read_root():
    return {"message": "Welcome to FastAPI with Middleware!"}

# Another endpoint to simulate a longer process
@app.get("/slow")
async def slow_endpoint():
    # Simulate a slow processing task
    time.sleep(3)  # Pauses execution for 2 seconds
    return {"message": "This is a slow response"}

# Endpoint that returns error to see middleware behavior
@app.get("/error")
async def error_endpoint():
    return JSONResponse(status_code=500, content={"message": "Something went wrong!"})



"""
Accessing the Endpoints: Open a browser or use a tool like curl or Postman to test the following endpoints:

http://127.0.0.1:8000/: This should give a fast response with a minimal X-Process-Time header value.
http://127.0.0.1:8000/slow: This endpoint will have a higher X-Process-Time due to the simulated delay.
http://127.0.0.1:8000/error: This will return a 500 status with the custom X-Process-Time header, demonstrating that the middleware applies regardless of success or error status.
"""


"""
Step-by-Step Execution Flow if i hit http://127.0.0.1:8000/slow.
1 -Receiving the Request:

You send an HTTP GET request to http://127.0.0.1:8000/slow.
FastAPI receives this request and immediately passes it to the middleware registered with @app.middleware("http").

2- Entering the Middleware (add_process_time_header):

Start Time Capture: The middleware function add_process_time_header begins by capturing the current time using time.perf_counter() and storing it in start_time.
python

start_time = time.perf_counter()
This high-precision timer provides a floating-point number representing the time in seconds, useful for measuring processing time accurately.


3-Calling the Endpoint Handler with call_next:

Invoke Next Handler: call_next is a special function provided by FastAPI in middleware functions. When you call await call_next(request), it hands over the request to the designated endpoint handler (/slow in this case) and waits for the response to come back.
python

response = await call_next(request)
At this point, FastAPI goes to execute the /slow endpoint handler while the middleware pauses to wait for the response.
Executing the /slow Endpoint Handler:

Simulating a Delay: Inside the /slow endpoint, there’s a time.sleep(2) statement, which introduces a 2-second delay.
python
time.sleep(2)  # Pauses execution for 2 seconds
This time.sleep(2) call blocks the function for 2 seconds, simulating a "slow" processing task.
4-Return the Response: After the delay, the handler prepares a response dictionary:
{
  "message": "This is a slow response"
}
FastAPI then converts this dictionary to JSON format, wraps it in a response object, and sends it back to the middleware.
5-Returning to Middleware After Endpoint Execution:

Capture the End Time: Back in the add_process_time_header middleware, time.perf_counter() is called again to capture the current time after the endpoint handler has finished.
python
process_time = time.perf_counter() - start_time
The difference between the new time and start_time gives the total processing time (process_time), which includes the 2-second delay and any minor overhead from handling the request.
6-Adding the Custom Header:

Setting X-Process-Time: The middleware adds a custom header X-Process-Time to the response, converting process_time to a string.
python
response.headers["X-Process-Time"] = str(process_time)
This header now contains the total time taken (in seconds) to process the /slow endpoint request, typically around 2.002 seconds (including processing overhead).
7-Returning the Final Response:

Response Sent to Client: The middleware returns the response back to FastAPI, which then sends it to the client (your browser or Postman).
The client receives:

{
  "message": "This is a slow response"
}
with headers:
arduino
X-Process-Time: 2.002  (approximately)


Summary of Key Points
Middleware Timing: The middleware wraps the request processing and captures timing data accurately before and after the handler runs.
Endpoint Delay: The delay (2 seconds) in the /slow endpoint is accurately reflected in the X-Process-Time header, which shows the actual time the server spent on processing.
Custom Header Addition: Middleware modifies the response object before it reaches the client, enabling detailed tracking of performance.
In essence, the middleware lets you measure and report on the server’s response time, including delays introduced within endpoint logic, providing invaluable data for performance monitoring and optimization.




GET /error: This endpoint returns a 500 error to demonstrate how middleware handles different types of responses, even errors.
The X-Process-Time will still be added to the response, allowing you to monitor request handling duration even when there are failures.
"""